{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science â€“ Homework 8\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Due: Friday, April 15, 11:59pm.\n",
    "\n",
    "In this homework, you will use clustering, principal component analysis, regular expressions, and natural language processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "First Name:\n",
    "<br>\n",
    "Last Name:\n",
    "<br>\n",
    "E-mail:\n",
    "<br>\n",
    "UID:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Analyze US Crime data\n",
    "\n",
    "We'll analyze a dataset describing 1973 violent crime rates by US State. The crimes considered are assault, murder, and rape. Also included is the percent of the population living in urban areas.\n",
    "\n",
    "The dataset is available as *USarrests.csv*. The dataset has 50 observations (corresponding to each state) on 4 variables: \n",
    "1. Murder: Murder arrests (per 100,000 residents)\n",
    "2. Assault: Assault arrests (per 100,000 residents)\n",
    "3. UrbanPop: Percent urban population\n",
    "4. Rape: Rape arrests (per 100,000 residents)\n",
    "\n",
    "\n",
    "You can read more about the dataset [here](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/USArrests.html). \n",
    "\n",
    "Our goal will be to use unsupervised methods to understand how violent crimes differ between states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 Import the data and perform some preliminary exploratory analysis. \n",
    "Use the *read_csv* pandas function to import the data as a dataframe. \n",
    "\n",
    "Plot a scatterplot matrix of the data. Explore basic statistics of the data. Write a few sentences describing how the variables are correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your description:** TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Cluster Heat Map\n",
    "\n",
    "Generate a [cluster heat map](https://seaborn.pydata.org/generated/seaborn.clustermap.html) with a dendrogram using seaborn (see lecture). Be sure to standardize the dataset using the `standard_scale=1` parameter.\n",
    "\n",
    "Describe any patterns you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 Visualize the data using PCA\n",
    "\n",
    "Complete the following steps:\n",
    "1. Scale the dataset using the *scale* function of the sklearn.preprocessing library. \n",
    "2. Calculate the principal components of the dataset. \n",
    "3. Store the principal components in a pandas dataframe. \n",
    "4. Plot a scatterplot of PC1 and PC2. Using the matplotlib function *annotate*, use the state names as markers (instead of dots).\n",
    "5.Print the explained variance ratio of the PCA. Plot the explained variance ratio of the PCA. Interpret these values. Is it reasonable to reduce the four dimensional space to two dimensions using PCA? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your description:** TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 k-means cluster analysis\n",
    "\n",
    "1. Using k-means, cluster the states into four clusters. Use the scaled dataset. Which states belong to which clusters?\n",
    "2. Vary k (between 2 and 20) and find the *best* value. Describe how do you determine *best*? \n",
    "3. Use the principal components to plot the best clustering. Again label each point using the state name and this time color the states according to the clustering. Is the PCA plot consistent with the clustering?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation for best K:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation for PCA and K-Means**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5 Hierarchical cluster analysis\n",
    "\n",
    "1.  Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states into four clusters. Which states belong to which clusters? \n",
    "2. Visualize your cluster results on top of the first two principle components, as before.\n",
    "3. Do you get similar results as for k-means? Can you see trends between the states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.6 DBSCAN\n",
    "\n",
    "1.  Using DBSCAN and experiment with different values for $\\epsilon$ and min samples. Which states belong to which clusters? \n",
    "2. Visualize your cluster results on top of the first two principle components, as before.\n",
    "3. Do you get similar results as before? Is DBSCAN stable or very sensitive to changes in epsilon for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regular Expressions \n",
    "\n",
    "Write regular expressions for the following examples that matches the data of the given format and any other reasonable variations thereof. E.g., your regex shouldn't be specific to one URL or one phone number, but should work for all examples of the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1.** Writes a regular expression that extracts the urls out of this string, but only the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To learn about pros/cons of data science, go to http://datascience.net.Alternatively, go to datascience.net/2018/\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"To learn about pros/cons of data science, go to http://datascience.net.\\\n",
    "Alternatively, go to datascience.net/2020/ or or datascience.net/2021/\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2.** Write a regular expression that extracts all phone numbers and fax numbers from this text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"You can reach me at 801-774-4321, or my office at (801) 223 9571 or (801) 223 2522.\\ \n",
    "Send me a fax at 857 188 7422. We finally made the sale for all 977 giraffes.\\\n",
    "They wanted 225 957 dollars for it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3.** Write a regular expression that extracts all opening html tags from this, including `<br />`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"This is <b>important</b> and <u>very</u><i>timely</i><br />. Was this <span> what you meant?</span>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4.** Write a regular expression that extracts all the names of people from the following text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Arnold Schwarzenegger was born in Austria. He and Sylvester Stalone used to run a restaurant\\\n",
    "with J. Edgar Hoover.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.** Write a regular expression that extracts the text out of all html elements of class important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Lorem ipsum dolor <b>sit</b> amet, <b class=\"important\">consectetur adipiscing</b> elit,\\ \n",
    "sed do eiusmod <span id=\"note\">tempor incididunt ut</span> <div>labore <strong class=\"important\">\\\n",
    "et dolore magna</strong> aliqua.</div> Ut enim ad minim veniam, quis nostrud exercitation ullamco.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP: Classifying Newsgroups Articles\n",
    "\n",
    "Newsgroups were the social media of the 90s. Newsgroups are open discussion forums structured into hierarchies. For example, the following newsgroups cover topics as divers as atheism, computer graphics, and classified ads.  \n",
    "\n",
    "```\n",
    "alt.atheism\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "misc.forsale\n",
    "```\n",
    "\n",
    "We will be combining machine learning and natural language processing to classify the news articles into these groups. We expect, for example, that the text for a classified ad in `misc.forsale` is different from text in `alt.atheism`. \n",
    "\n",
    "We will use the 20 Newsgroups corpus from scikit-learn. The 20 newsgroups dataset comprises around 18,000 newsgroups posts on 20 topics. The general steps we follow are:\n",
    "1. Load the corpus    \n",
    "2. Do preprocessing: removal of stopwords, stemming, etc.\n",
    "3. Vectorize the text\n",
    "4. Split into training and test sets\n",
    "5. Train our classifier\n",
    "\n",
    "Refer to documentation on the [20 newsgroups dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) to learn about the dataset and find out how to download it.\n",
    "We recommended you use the `subset='all'` parameter to load all the data at once, instead of `subset='train'` and `subset='test'` separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1.** Load the dataset.\n",
    "\n",
    "1. Print the exact number of news articles in the corpus.\n",
    "2. Print all 20 categories the news articles belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 Classification\n",
    "\n",
    "Vectorize the data using vectorizers from sklearn. Using these vectors as features and the article category from corpus as labels, train a NaiveBayes classifier to classify the data.\n",
    "\n",
    "#### Vectorizers\n",
    "\n",
    "Vectorizes help us to transform text data into features we can use in machine learning. We did the vectorization manually in class, here you will use pre-build vectorizers. \n",
    "\n",
    "You should use CountVectorizer and TfidfVectorizer vectorizers from sklearn to vectorize your data. Please refer to documentation on both to learn how to use them.\n",
    "+ http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "+ http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Compare the performance of classifiers using both vectorizers. You are encouraged to experiment with different parameters like max_df, min_df, etc. See docs for the meanings.\n",
    "\n",
    "#### Naive Bayes\n",
    "**Resources**\n",
    "1. https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "2. https://www.geeksforgeeks.org/naive-bayes-classifiers\n",
    "3. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "We will be using Multinomial Naive Bayes from sklearn. Refer to documentation above for how to import the classifier. Then it can be used like any other classifier by using fit and predict functions provided on it.\n",
    "e.g:\n",
    "\n",
    "```\n",
    "clf = MulitnomialNB(alpha = 1)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "```\n",
    "\n",
    "Alpha is also known as the smoothing factor and ranges from 0 (no smoothing) to 1 (Laplace Smoothing). You can experiment with different values to see if you get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3 Removing Stopwords\n",
    "\n",
    "Now we'll use the NLTK stopword list to improve our data vectors. TfidfVectorizer and CountVectorizer both can take an argument called stop_words. The words passed to this argument are considered as stopwords and are not vectorized. Use the stopwords list from NLTK and pass it to vectorizers. Then evaluate the new vectors using Multinomial Naive Bayes.\n",
    "\n",
    "Answer the following questions:\n",
    "1. How much accuracy would a naive approach, that picks one of the 20 categories at random achieve?\n",
    "1. What accuracy were you able to achieve? \n",
    "1. What was the influence of the different vectorizers and the stopword removal? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
